---
layout: about
title: about
permalink: /
subtitle: explainable, fair, privacy-preserving, causal, and robust machine learning research
order: 1

news: false  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
#profile:
#  align: right
#  image: blank.png
#  address: >
#    <p>Paper Highlight</p>
#    <p>Jane Doe PhD</p>
#    <p>University of X</p>
social: false # includes social icons at the bottom of the page
---
As machine learning (ML) systems are increasingly being deployed in real-world applications, it is critical to ensure that these systems are behaving responsibly and are trustworthy.  To this end, there has been growing interest from researchers and practitioners to develop and deploy ML models and algorithms that are not only accurate, but also explainable, fair, privacy-preserving, causal, and robust. This broad area of research is commonly referred to as trustworthy ML.   
  
While it is incredibly exciting that researchers from diverse domains ranging from machine learning to health policy and law are working on trustworthy ML, this has also resulted in the emergence of critical challenges such as information overload and lack of visibility for work of early career researchers. Furthermore, the barriers to entry into this field are growing day-by-day -- researchers entering the field are faced with an overwhelming amount of prior work without a clear roadmap of where to start and how to navigate the field.   
  
To address these challenges, we are launching the Trustworthy ML Initiative (TrustML) with the following goals:   
1. Enable easy access of fundamental resources to newcomers in the field. 
2. Provide a platform for early career researchers to showcase and disseminate their work.
3. Keep you up to date on current research in this field
4. Highlight trustworthy research during conferences and other events

